Título: Historia de la inteligencia artificial - Wikipedia, la enciclopedia libre
URL: https://es.wikipedia.org/wiki/Historia_de_la_inteligencia_artificial
Número de palabras: 3450

La historia de la inteligencia artificial (IA) comenzó en la antigüedad, con mitos, historias y rumores sobre seres artificiales dotados de inteligencia o conciencia por parte de maestros artesanos. Las semillas de la IA moderna fueron plantadas por filósofos que intentaron describir el proceso del pensamiento humano como la manipulación mecánica de símbolos. Este trabajo culminó con la invención de la computadora digital programable en la década de 1940, una máquina basada en la esencia abstracta del razonamiento matemático. Este dispositivo y las ideas detrás de él inspiraron a un puñado de científicos a comenzar a discutir seriamente la posibilidad de construir un cerebro electrónico.
 El campo de la investigación de la IA se fundó en un taller celebrado en el campus del Dartmouth College, en Estados Unidos, durante el verano de 1956.[1]​Aquellos que asistieron se convertirían en los líderes de la investigación en IA durante décadas. Muchos de ellos predijeron que una máquina tan inteligente como un ser humano existiría en no más de una generación, y recibieron millones de dólares para hacer realidad esta visión.[2]​
 Al final, resultó evidente que los investigadores habían subestimado enormemente la dificultad del proyecto. En 1974, en respuesta a las críticas de James Lighthill y a la presión constante del Congreso de Estados Unidos, los gobiernos de Estados Unidos y Gran Bretaña dejaron de financiar investigaciones no dirigidas sobre inteligencia artificial. Siete años más tarde, una iniciativa visionaria del gobierno japonés inspiró a los gobiernos y a la industria a proporcionar a la IA miles de millones de dólares, pero a finales de la década de 1980 los inversores se desilusionaron y volvieron a retirar la financiación. Los años difíciles que siguieron se conocerían más tarde como el «invierno de la IA» (AI Winter en inglés). La IA fue criticada en la prensa y evitada por la industria hasta mediados de la década de 2000, pero la investigación y la financiación continuaron creciendo bajo otros nombres.
 En los años 1990 y principios de 2000, el aprendizaje automático se aplicó a muchos problemas en la academia y la industria. El éxito se debió a la disponibilidad de hardware informático potente, la recopilación de conjuntos de datos inmensos y la aplicación de sólidos métodos matemáticos. En 2012, el aprendizaje profundo demostró ser una tecnología revolucionaria, eclipsando todos los demás métodos. La arquitectura del transformador debutó en 2017 y se utilizó para producir aplicaciones de IA generativa. La inversión en IA se disparó en la década de 2020.
 En la mitología griega, Talos era un gigante construido en bronce que actuaba como guardián de la isla de Creta. Lanzaba piedras a los barcos de los invasores y completaba 3 circuitos alrededor del perímetro de la isla diariamente.[3]​Según la Bibliotheke de Pseudo-Apollodoro, Hefesto forjó a Talos con la ayuda de un cíclope y presentó el autómata como regalo a Minos.[4]​ En la Argonautica, Jasón y los Argonautas lo derrotaron mediante un único tapón cerca de su pie que, una vez retirado, permitía que el vital ichor fluyera fuera de su cuerpo, dejándolo inanimado.[5]​
 Pigmalión fue un legendario rey y escultor de la mitología griega, famoso por su representación en las Metamorfosis de Ovidio. En el décimo libro del poema narrativo de Ovidio, Pigmalión se disgusta con las mujeres al presenciar la forma en que las Propoetides se prostituyen.[6]​  A pesar de esto, hace ofrendas en el templo de Venus pidiendo a la diosa que le traiga una mujer exactamente como una estatua que él esculpió.
 En De la Naturaleza de las Cosas, escrito por el alquimista suizo Paracelso, describe un procedimiento que afirma puede fabricar un «hombre artificial». Al colocar el «esperma de un hombre» en estiércol de caballo y alimentarlo con el «Arcano de la sangre humana» después de 40 días, la mezcla se convertirá en un infante vivo.[7]​
 El relato escrito más antiguo sobre la fabricación de golems se encuentra en los escritos de Eleazar ben Judá de Worms a principios del siglo XIII.[8]​Durante la Edad Media, se creía que la animación de un Golem se podía lograr insertando un trozo de papel con cualquiera de los nombres de Dios en la boca de la figura de arcilla.[9]​A diferencia de los autómatas legendarios como Brazen Heads,[10]​ un Golem no podía hablar.[11]​
 Takwin, la creación artificial de vida, era un tema frecuente en los manuscritos alquímicos ismaelitas, especialmente los atribuidos a Jabir ibn Hayyan. Los alquimistas islámicos intentaron crear una amplia gama de formas de vida a través de su trabajo, desde plantas hasta animales.[12]​
 En Fausto: La segunda parte de la tragedia de Johann Wolfgang von Goethe, un homúnculo fabricado alquímicamente, destinado a vivir eternamente en el frasco en el que fue creado, se esfuerza por nacer en un cuerpo humano completo. Sin embargo, al inicio de esta transformación, el frasco se rompe y el homúnculo muere.[13]​
 Para el siglo XIX, las ideas sobre hombres artificiales y máquinas pensantes se desarrollaron en la ficción, como en «Frankenstein» de Mary Shelley o «R.U.R.» (Rossum's Universal Robots) de Karel Čapek,[14]​ y en la especulación, como en «Darwin among the Machines» de Samuel Butler,[15]​ y en casos del mundo real, incluyendo «Maelzel's Chess Player» de Edgar Allan Poe. La IA es un tema común en la ciencia ficción hasta el presente.[16]​
 La inteligencia artificial se basa en la suposición de que el proceso del pensamiento humano puede ser mecanizado. El estudio del razonamiento mecánico o «formal» tiene una larga historia. Los filósofos chinos, indios y griegos desarrollaron todos métodos estructurados de deducción formal para el primer milenio antes de Cristo. Sus ideas fueron desarrolladas a lo largo de los siglos por filósofos como Aristóteles (quien dio un análisis formal del silogismo),[17]​Euclides (cuyos elementos fueron un modelo de razonamiento formal), al-Juarismi (quien desarrolló el álgebra y dio su nombre a la palabra «algoritmo») y filósofos escolásticos europeos como Guillermo de Ockham y Duns Scoto.[18]​
 El filósofo español Ramon Llull (1232-1315) desarrolló varias máquinas lógicas dedicadas a la producción de conocimiento por medios lógicos;[19]​[20]​Llull describió sus máquinas como entidades mecánicas que podían combinar verdades básicas e innegables mediante operaciones lógicas simples, producidas por la máquina mediante significados mecánicos, de tal manera que produjeran todo el conocimiento posible.[21]​La obra de Llull tuvo una gran influencia en Gottfried Leibniz, quien reformuló sus ideas.[22]​
 En el siglo XVII, Leibniz, Thomas Hobbes y René Descartes exploraron la posibilidad de que todo pensamiento racional pudiera volverse tan sistemático como el álgebra o la geometría.[23]​Hobbes escribió en Leviatán: «Porque la razón... no es más que calcular, es decir, sumar y restar».[24]​Leibniz imaginó un lenguaje universal del razonamiento, la characteristica universalis, que reduciría la argumentación a cálculo, de modo que «no habría más necesidad de disputa entre dos filósofos que entre dos contables. Porque bastaría tomar sus lápices en la mano, bajar a sus pizarras y decirse el uno al otro (con un amigo como testigo, si quisieran): Vamos a calcular».[25]​Estos filósofos habían comenzado a articular la hipótesis del sistema de símbolos físicos que se convertiría en la fe rectora de la investigación de la IA.
 El estudio de la lógica matemática proporcionó el avance esencial que hizo que la inteligencia artificial pareciera plausible. Las bases se habían sentado con obras como Las leyes del pensamiento de Boole y Begriffsschrift de Frege.[26]​Basándose en el sistema de Frege, Russell y Whitehead, presentaron un tratamiento formal de los fundamentos de las matemáticas en su obra maestra, los Principia Mathematica, en 1913. Inspirado por el éxito de Russell, David Hilbert desafió a los matemáticos de los años 20 y 30 a responder esta pregunta fundamental: «¿puede formalizarse todo el razonamiento matemático?».[18]​Su pregunta fue respondida por la prueba de incompletitud de Gödel,[27]​ la máquina de Turing[27]​ y el cálculo Lambda de Church.[29]​
 Su respuesta fue sorprendente en dos aspectos. Primero, demostraron que, de hecho, existían límites a lo que la lógica matemática podía lograr. Pero segundo (y más importante para la IA), su trabajo sugirió que, dentro de estos límites, cualquier forma de razonamiento matemático podía ser mecanizada. La tesis de Church-Turing implicaba que un dispositivo mecánico, manipulando símbolos tan simples como 0 y 1, podía imitar cualquier proceso concebible de deducción matemática.[27]​La idea clave fue la máquina de Turing, una construcción teórica simple que capturó la esencia de la manipulación de símbolos abstractos.[30]​Este invento inspiraría a un puñado de científicos a comenzar a discutir la posibilidad de máquinas pensantes.
 Las máquinas calculadoras fueron diseñadas o construidas en la antigüedad y a lo largo de la historia por muchas personas, entre ellas Gottfried Leibniz, Joseph Marie Jacquard, Charles Babbage, Percy Ludgate, Leonardo Torres Quevedo, Vannevar Bush, y otros. Ada Lovelace especuló que la máquina de Babbage era «una máquina pensante o... razonadora», pero advirtió que «es deseable protegerse contra la posibilidad de que surjan ideas exageradas en cuanto a los poderes de la maquina».[31]​[32]​
 Los primeros computadores modernos fueron las máquinas masivas de la Segunda Guerra Mundial (como la Z3 de Konrad Zuse, la Heath Robinson y la Colossus de Alan Turing, y la Berry en la Universidad de Pensilvania).[33]​ La ENIAC se basó en los fundamentos teóricos establecidos por Alan Turing y desarrollada por John von Neumann,[34]​ y demostró ser la más influyente.[33]​
 Las primeras investigaciones sobre máquinas pensantes se inspiraron en una confluencia de ideas que se volvieron prevalentes a finales de la década de 1930, 1940 y principios de la de 1950. Investigaciones recientes en neurología habían demostrado que el cerebro era una red eléctrica de neuronas que disparaban en pulsos todo o nada. La cibernética de Norbert Wiener describió el control y la estabilidad en las redes eléctricas. La teoría de la información de Claude Shannon describió las señales digitales (es decir, señales «todo o nada»). La teoría de la computación de Alan Turing demostró que cualquier forma de computación podía describirse digitalmente. La estrecha relación entre estas ideas sugirió que podría ser posible construir un «cerebro electrónico».
 En las décadas de 1940 y 1950, un puñado de científicos de diversos campos (matemáticas, psicología, ingeniería, economía y ciencias políticas) exploraron varias direcciones de investigación que serían vitales para la investigación posterior de la IA.[35]​Alan Turing fue uno de los primeros en investigar seriamente la posibilidad teórica de la «inteligencia artificial». El campo de la «investigación en inteligencia artificial» se fundó como disciplina académica en 1956.[36]​[37]​
 En 1950, Turing publicó un artículo histórico denominado «Computing Machinery and Intelligence», en el que especulaba sobre la posibilidad de crear máquinas que piensen.[38]​[39]​En el artículo, señaló que «pensar» es difícil de definir e ideó su famosa prueba de Turing: Si una máquina pudiera mantener una conversación (a través de un teletipo) que fuera indistinguible de una conversación con un ser humano, entonces era razonable decir que la máquina estaba «pensando».[40]​Esta versión simplificada del problema permitió a Turing argumentar de manera convincente que una «máquina pensante» era al menos plausible y el artículo respondió a todas las objeciones más comunes a la proposición.[41]​El Test de Turing fue la primera propuesta seria en la filosofía de la inteligencia artificial.
 
 Walter Pitts y Warren McCulloch analizaron redes de neuronas artificiales idealizadas y mostraron cómo podrían realizar funciones lógicas simples en 1943.[42]​[43]​Fueron los primeros en describir lo que los investigadores posteriores llamarían una red neuronal.[44]​El artículo fue influenciado por el artículo de Turing «Sobre números computables» de 1936 utilizando neuronas booleanas de dos estados similares, pero fue el primero en aplicarlo a la función neuronal.[36]​Uno de los estudiantes inspirados por Pitts y McCulloch fue Marvin Minsky, que en ese momento era un estudiante de posgrado de 24 años. En 1951, Minsky y Dean Edmonds construyeron la primera máquina de red neuronal, la SNARC.
 En la década de 1950, se construyeron robots experimentales como las tortugas de W. Gray Walter y la Bestia de Johns Hopkins. Estas máquinas no utilizaban computadoras, electrónica digital ni razonamiento simbólico; estaban controlados completamente por circuitos analógicos.
 En 1951, utilizando la máquina Ferranti Mark 1 de la Universidad de Mánchester, Christopher Strachey escribió un programa de damas y Dietrich Prinz uno de ajedrez.[45]​El programa de damas de Arthur Samuel, tema de su artículo de 1959 «Algunos estudios sobre aprendizaje automático utilizando el juego de damas», finalmente alcanzó la habilidad suficiente para desafiar a un experto.[46]​
 Cuando el acceso a las computadoras digitales se hizo posible a mediados de los cincuenta, algunos científicos reconocieron instintivamente que una máquina que podía manipular números también podía manipular símbolos, y que la manipulación de símbolos bien podría ser la esencia del pensamiento humano. Este fue un nuevo enfoque para crear máquinas pensantes.[47]​
 En 1955, Allen Newell y el futuro Premio Nobel Herbert A. Simon crearon el «Logic Theorist», con la ayuda de J. C. Shaw. El programa eventualmente demostraría 38 de los primeros 52 teoremas en Principia Mathematica de Russell y Whitehead, y encontraría pruebas nuevas y más elegantes para algunos.[48]​Simon dijo que habían «resuelto el venerable problema mente/cuerpo, explicando cómo un sistema compuesto de materia puede tener las propiedades de la mente».[49]​Esta fue una de las primeras declaraciones de la posición filosófica que John Searle más tarde llamaría «IA fuerte»: que las máquinas pueden contener mentes tal como lo hacen los cuerpos humanos.[50]​
 En el otoño de 1956, Newell y Simon también presentaron el Logic Theorist en una reunión del Grupo de Interés Especial en Teoría de la Información en el Instituto Tecnológico de Massachusetts (MIT). En la misma reunión, Noam Chomsky discutió su gramática generativa, y George Miller describió su artículo histórico «El número mágico siete, más o menos dos». Miller escribió: «Salí del simposio con la convicción, más intuitiva que racional, de que la psicología experimental, la lingüística teórica y la simulación por computadora de los procesos cognitivos eran todas piezas de un todo más grande».[51]​
 Esta reunión marcó el inicio de la «revolución cognitiva», un cambio de paradigma interdisciplinario en psicología, filosofía, ciencias de la computación y neurociencia. Inspiró la creación de los subcampos de inteligencia artificial simbólica, lingüística generativa, ciencia cognitiva, psicología cognitiva, neurociencia cognitiva y las escuelas filosóficas del computacionalismo y el funcionalismo. Todos estos campos utilizaron herramientas relacionadas para modelar la mente, y los resultados descubiertos en uno de ellos eran relevantes para los demás.
 El enfoque cognitivo permitió a los investigadores considerar «objetos mentales» como pensamientos, planes, metas, hechos o recuerdos, a menudo analizados utilizando símbolos de alto nivel en redes funcionales. Estos objetos habían sido prohibidos como «inobservables» por paradigmas anteriores como el conductismo. Los objetos mentales simbólicos se convertirían en el principal foco de investigación y financiación de la IA durante las siguientes décadas.
 Los programas desarrollados en los años posteriores al Taller de Dartmouth fueron, para la mayoría de la gente, simplemente «asombrosos»:[52]​Las computadoras resolvían problemas planteados de álgebra, demostraban teoremas de geometría y aprendían a hablar inglés. Pocos en aquel momento habrían creído que un comportamiento tan «inteligente» por parte de las máquinas fuera posible.[53]​Los investigadores expresaron un intenso optimismo en privado y en la prensa, prediciendo que se construiría una máquina totalmente inteligente en menos de 20 años.[54]​Agencias gubernamentales como la Agencia de Proyectos de Investigación Avanzada de Defensa (DARPA, entonces conocida como «ARPA») invirtieron dinero en este campo. A finales de los años cincuenta y principios de los sesenta se crearon laboratorios de inteligencia artificial en varias universidades británicas y estadounidenses.[36]​
 Hubo muchos programas exitosos y nuevas direcciones a finales de los años cincuenta y sesenta. 
 Muchos de los primeros programas de IA utilizaban el mismo algoritmo básico. Para alcanzar algún objetivo (como ganar un juego o demostrar un teorema), procedían paso a paso hacia él (haciendo un movimiento o una deducción) como si estuvieran buscando en un laberinto, retrocediendo cada vez que llegaban a un callejón sin salida.  
 La principal dificultad era que, para muchos problemas, el número de posibles caminos a través del «laberinto» era astronómico (una situación conocida como «explosión combinatoria»). Los investigadores reducían el espacio de búsqueda utilizando heurísticas que eliminaban caminos que parecían poco probables de llevar a una solución.[55]​
 Newell y Simon intentaron capturar una versión general de este algoritmo en un programa llamado «General Problem Solver».[56]​Otros programas de 'búsqueda' pudieron lograr tareas impresionantes como resolver problemas de geometría y álgebra, como el Geometry Theorem Prover (1958) de Herbert Gelernter y el Symbolic Automatic Integrator (SAINT), escrito por James Slagle, estudiante de Minsky, en 1961.[57]​Otros programas buscaban metas y submetas para planificar acciones, como el sistema STRIPS desarrollado en Stanford para controlar el comportamiento del robot Shakey.[58]​
 Un objetivo importante de la investigación de la IA es permitir que las computadoras se comuniquen en lenguajes naturales como el inglés. Uno de los primeros éxitos fue el programa STUDENT de Daniel Bobrow, que podía resolver problemas planteados de álgebra de la escuela secundaria.[59]​
 Una red semántica representa conceptos (por ejemplo, «casa», «puerta») como nodos, y relaciones entre conceptos como vínculos entre los nodos. El primer programa de IA que utilizó una red semántica fue escrito por Ross Quillian,[60]​y la versión más exitosa (y controvertida) fue la teoría de la dependencia conceptual de Roger Schank.[60]​
 El programa informático ELIZA fue el primer chatbot que podía llevar a cabo conversaciones tan realistas que los usuarios ocasionalmente eran engañados al pensar que estaban comunicándose con un ser humano y no con un programa de computadora. Pero en realidad, ELIZA simplemente daba una respuesta enlatada o repetía lo que se le decía, reformulando su respuesta con algunas reglas gramaticales. [61]​
 A finales de los años 60, Marvin Minsky y Seymour Papert, del Laboratorio de IA del MIT, propusieron que la investigación en IA debería centrarse en situaciones artificialmente simples conocidas como micromundos. Señalaron que en ciencias exitosas como la física, los principios básicos a menudo se entendían mejor utilizando modelos simplificados como planos sin fricción o cuerpos perfectamente rígidos. Gran parte de la investigación se centró en un «mundo de bloques», que consiste en bloques de colores de diversas formas y tamaños dispuestos sobre una superficie plana.[62]​
 Este paradigma condujo a un trabajo innovador en visión por computadora por parte de Gerald Sussman, Adolfo Guzmán, David Waltz (quien inventó la «propagación de restricciones») y, especialmente, Patrick Winston. Al mismo tiempo, Minsky y Papert construyeron un brazo robótico que podía apilar bloques, dando vida al mundo de los bloques. El programa SHRDLU de Terry Winograd podía comunicarse en oraciones comunes en inglés sobre el micromundo, planificar operaciones y ejecutarlas.[63]​
 La primera generación de investigadores de IA hizo estas predicciones sobre su trabajo:
 En junio de 1963, el MIT recibió una subvención de $2.2 millones de la recién creada Agencia de Proyectos de Investigación Avanzada (ARPA, más tarde conocida como DARPA). El dinero se utilizó para financiar el proyecto MAC, que absorbió el "Grupo de IA" fundado por Minsky y McCarthy cinco años antes. DARPA continuó proporcionando $3 millones cada año hasta la década de 1970.[68]​
 La inteligencia artificial es una herramienta muy poderosa que puede traer muchos beneficios a la sociedad, como la automatización de tareas repetitivas, el diagnóstico médico preciso, la personalización de la publicidad y la reducción del tiempo de espera en las llamadas de atención al cliente. Sin embargo, también puede presentar desafíos y preocupaciones importantes.
 Uno de los principales desafíos de la inteligencia artificial es la falta de transparencia en su funcionamiento. En muchos casos, los algoritmos de aprendizaje automático son considerados como cajas negras, lo que significa que los usuarios no pueden comprender cómo se toman las decisiones. Esto puede ser un problema en aplicaciones críticas, como la toma de decisiones médicas y financieras.
 Otro desafío importante es la privacidad y seguridad de los datos. La inteligencia artificial se basa en grandes conjuntos de datos para entrenar modelos de aprendizaje automático. Si estos datos son robados o utilizados de manera inapropiada, pueden tener graves consecuencias para la privacidad y la seguridad de las personas.
 Además, la inteligencia artificial también puede exacerbar la desigualdad social y económica. Por ejemplo, los sistemas de inteligencia artificial que se basan en datos históricos pueden perpetuar sesgos y discriminación en los procesos de toma de decisiones. Además, los avances en la automatización de trabajos pueden llevar a la eliminación de puestos de trabajo, lo que puede agravar la brecha económica.
 Otra preocupación importante es la responsabilidad y ética de los sistemas de inteligencia artificial. Quienes diseñan, implementan y utilizan estos sistemas tienen la responsabilidad de garantizar que se utilicen de manera ética y responsable. Además, se deben establecer regulaciones y estándares éticos para la inteligencia artificial para garantizar que no se utilice de manera perjudicial para las personas y la sociedad en general.
 En resumen, la inteligencia artificial[69]​ presenta importantes desafíos y preocupaciones que deben abordarse de manera responsable y ética. Es necesario seguir investigando y desarrollando regulaciones y estándares éticos para garantizar que la inteligencia artificial se utilice de manera responsable y para el beneficio de la sociedad en general.
