Título: Uso de modelos de Machine Learning en la aplicaciÃ³n de Windows | Microsoft Learn
URL: https://learn.microsoft.com/es-es/windows/ai/models
Número de palabras: 1447

Este explorador ya no se admite. Actualice a MicrosoftÂ Edge para aprovechar las caracterÃ­sticas y actualizaciones de seguridad mÃ¡s recientes, y disponer de soporte tÃ©cnico. Esta guÃ­a pretende ayudar a los desarrolladores de aplicaciones sin experiencia en trabajar con modelos de inteligencia artificial (IA) y Machine Learning (ML), ya que aborda preguntas habituales, comparte conceptos y recursos bÃ¡sicos y ofrece recomendaciones sobre cÃ³mo usar modelos de IA y ML en una aplicaciÃ³n de Windows. Machine Learning (ML) es una ramificaciÃ³n de la inteligencia artificial (IA) que permite a los ordenadores aprender de los datos y realizar predicciones o tomar decisiones. Los modelos de ML son algoritmos que se pueden entrenar con datos y, posteriormente, implementar para realizar diversas tareas, como generar contenido, razonar sobre contenido, reconocer imÃ¡genes, procesar el lenguaje natural, realizar anÃ¡lisis de sentimiento y mucho mÃ¡s. Formas en que las aplicaciones de Windows pueden sacar provecho de los modelos de ML para mejorar su funcionalidad y la experiencia del usuario: Windows Copilot Runtime combina varias maneras de interactuar con el sistema operativo que utiliza la IA. Esto incluye caracterÃ­sticas y API respaldadas por IA y listas para usar, lo que llamamos la Windows Copilot Library. Consulte IntroducciÃ³n al uso de API respaldadas por IA en la aplicaciÃ³n de Windows  para ver una guÃ­a sobre estas caracterÃ­sticas y API listas para usar que brindan apoyo para algunos de los escenarios mencionados con anterioridad. Los modelos de la Windows Copilot Library se ejecutan de forma local, directamente en el dispositivo Windows, aunque tambiÃ©n puede optar por usar un modelo basado en la nube a travÃ©s de una API lista para usar. Tanto si ejecutan una instancia local como en la nube, estas API abstraen el modelo de ML subyacente para que no usted tenga que optimizar, formatear ni ajustar nada. Sin embargo, es posible que quiera encontrar su propio modelo de ML para usarlo localmente en Windows. Es posible que tenga que optimizar este modelo para que se ejecute correctamente en los dispositivos Windows o ajustar un modelo para entrenarlo con sus propios datos personalizados especÃ­ficos para el caso de uso o la empresa concretos. En este artÃ­culo se tratan algunos de los conceptos, herramientas y bibliotecas de cÃ³digo abierto que le guiarÃ¡n a travÃ©s de este proceso. Los modelos de lenguaje pequeÃ±os (SLM) estÃ¡n diseÃ±ados para ser compactos y eficientes, a menudo se entrenan para tareas o dominios especÃ­ficos en conjuntos de datos mÃ¡s pequeÃ±os para permitir almacenar y ejecutar el modelo a nivel local con un tiempo de rendimiento para la inferencia mÃ¡s rÃ¡pido. Los SLM cuentan con una restricciÃ³n en la cantidad de datos que se usan para entrenarlos, por lo que no proporciona un conocimiento o un razonamiento complejo tan amplio como un modelo de lenguaje grande (LLM). Sin embargo, los SLM pueden ofrecer una alternativa mÃ¡s segura y rentable que los LLM cuando se usan localmente, ya que requieren menos potencia de cÃ¡lculo para ejecutarse y la privacidad de los datos es mayor, al mantener la informaciÃ³n de la conversaciÃ³n segura a nivel local en el dispositivo. Los SLM son mÃ¡s idÃ³neos para el uso local, ya que ejecutar un modelo de ML en un dispositivo significa que el tamaÃ±o no debe superar la capacidad de almacenamiento y procesamiento del dispositivo que lo ejecuta. La mayorÃ­a de los LLM serÃ­an demasiado grandes para ejecutarse localmente. Los modelos Phi-2 y Phi-3 de Microsoft son ejemplos de SLM. Los modelos de lenguaje grandes (LLM) se han entrenado con grandes cantidades de datos y un mayor nÃºmero de parÃ¡metros, por lo que son mÃ¡s complejos y tienen un mayor tamaÃ±o para el almacenamiento. Debido a su tamaÃ±o, es posible que los LLM dispongan de una mayor capacidad para comprender patrones mÃ¡s complejos y con mÃ¡s matices en los datos, de modo que abarcan un espectro de conocimiento mÃ¡s amplio y permiten trabajar con patrones mÃ¡s complejos. TambiÃ©n requieren mayores recursos computacionales para el entrenamiento y la inferencia. La mayorÃ­a de los LLM no podrÃ­an ejecutarse en un dispositivo local. Los modelos de lenguaje OpenAI GPT-4o, GPT-4 Turbo, GPT-3.5 Turbo, DALL-E y Whisper son ejemplos de LLM. Para obtener mÃ¡s instrucciones sobre la diferencia entre usar un SLM localmente frente a un LLM en la nube, consulte Consideraciones a tener en cuenta para usar las API respaldadas por IA locales frente a las basadas en la nube en la aplicaciÃ³n de Windows. Los modelos de ML de cÃ³digo abierto que estÃ¡n listos para usarse y que se pueden personalizar con sus propios datos o preferencias estÃ¡n disponibles en diversos lugares, de los cuales algunos de los mÃ¡s populares son: Algunas bibliotecas de modelos no estÃ¡n diseÃ±adas para personalizarse y distribuirse a travÃ©s de una aplicaciÃ³n, pero son herramientas Ãºtiles para explorar y descubrir de manera prÃ¡ctica como parte del ciclo de vida de desarrollo, como por ejemplo: Siempre que encuentre un modelo de ML con el objetivo de usarlo en la aplicaciÃ³n de Windows, le recomendamos encarecidamente seguir las instrucciones de Desarrollo de caracterÃ­sticas y aplicaciones de IA generativa responsables en Windows. Esta guÃ­a le ayudarÃ¡ a comprender las directivas, los procedimientos y los procesos de gobernanza, identificar los riesgos, recomendar mÃ©todos de prueba y usar medidas de seguridad como moderadores y filtros, y expone consideraciones especÃ­ficas a la hora de seleccionar un modelo seguro y responsable con el que trabajar. Hay diferentes maneras de usar modelos de ML en aplicaciones de Windows, segÃºn el tipo, el origen y el formato de los modelos, y el tipo de aplicaciÃ³n. Algunos de los formatos en los que encontrarÃ¡ los modelos de ML son: ONNX: estÃ¡ndar abierto para representar e intercambiar modelos de ML en distintos marcos y plataformas. Si encuentra un modelo de ML previamente entrenado en formato ONNX, puede usar ONNX Runtime (ORT) para cargar y ejecutar el modelo en la aplicaciÃ³n de Windows. ORT permite acceder a las funcionalidades de inferencia aceleradas por hardware del dispositivo y optimizar el rendimiento del modelo de ML. Si tiene un modelo de ML previamente entrenado en un formato diferente, como PyTorch o TensorFlow, puede convertirlo a ONNX mediante una herramienta de optimizaciÃ³n de modelos como Olive. Para obtener ayuda con el uso de Olive, consulte Ajuste de SLM con Microsoft Olive (Serie de recorridos sobre la arquitectura de aplicaciones con IA generativa). Para ver tutoriales sobre cÃ³mo crear y usar modelos ONNX, consulte Tutoriales de ONNX en GitHub. Para ver ejemplos que muestren cÃ³mo usar modelos ONNX en una aplicaciÃ³n de Windows, consulte la GalerÃ­a de ejemplos de IA en Windows. PyTorch: un marco de aprendizaje profundo de cÃ³digo abierto muy popular disponible con interfaz de Python y C++. Es probable que este sea el formato mÃ¡s comÃºn que encontrarÃ¡ para los modelos de ML. Si quiere usar modelos de Ml de PyTorch en la aplicaciÃ³n de Windows (C# o C++) o en una aplicaciÃ³n web, puede usar TorchSharp y LibTorch, que son enlaces de .NET y C++ para la biblioteca de PyTorch. TorchSharp y LibTorch permiten crear, cargar y manipular tensores, compilar y ejecutar redes neuronales y guardar y cargar modelos mediante el formato PyTorch. Para obtener ejemplos, consulte los ejemplos de TorchSharp, TorchScript para implementaciÃ³n y ejemplos de C++ de PyTorch. En el caso de las aplicaciones web, consulte CreaciÃ³n de una aplicaciÃ³n web con ONNX Runtime. Para ver ejemplos de cÃ³mo ejecutar modelos de PyTorch con DirectML, consulte la galerÃ­a de ejemplos de IA en Windows. TensorFlow es otra conocida biblioteca de software de cÃ³digo abierto para el aprendizaje automÃ¡tico y la inteligencia artificial que se usan para compilar e implementar modelos de Machine Learning para diversas tareas. API de WebNN para aplicaciones web: estÃ¡ndar web para acceder a la aceleraciÃ³n de hardware de red neuronal en exploradores, basado en las API de WebIDL y JavaScript. Permite a los desarrolladores web crear y ejecutar modelos de Machine Learning de forma eficaz en el cliente, sin depender de servicios en la nube o bibliotecas nativas. Ejemplos de WebNN en GitHub. Ejemplos de WebNN con ONNX Runtime en la galerÃ­a de ejemplos de IA en Windows. AI Toolkit para Visual Studio Code es una extensiÃ³n de VS Code que le permite descargar y ejecutar modelos de IA localmente. AI Tookit tambiÃ©n puede ayudarle con: DirectML es una API de bajo nivel que permite que el hardware del dispositivo Windows acelere el rendimiento de los modelos de ML mediante la GPU o la NPU del dispositivo. El enlace de DirectML con el ONNX Runtime suele ser la forma mÃ¡s sencilla que los desarrolladores usan para llevar la IA acelerada mediante hardware a sus usuarios a gran escala. MÃ¡s informaciÃ³n: IntroducciÃ³n a DirectML. Â¿Le ha resultado Ãºtil esta pÃ¡gina?